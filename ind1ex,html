<!DOCTYPE html>
<html lang="bs">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>AI Voice Assistant</title>

<style>
  body {
    font-family: Arial, sans-serif;
    background: #111;
    color: #fff;
    text-align: center;
    padding: 40px;
  }
  button {
    padding: 15px 30px;
    font-size: 18px;
    border: none;
    border-radius: 10px;
    cursor: pointer;
    background: #0af;
    color: #fff;
  }
  #responseBox {
    margin-top: 30px;
    padding: 20px;
    background: #222;
    border-radius: 10px;
    font-size: 20px;
  }
</style>
</head>

<body>

<h1>üé§ AI Voice Assistant</h1>
<p>Klikni dugme i priƒçaj ‚Äî sistem slu≈°a i ≈°alje meni odgovor.</p>

<button id="talkBtn">üéôÔ∏è Priƒçaj sada</button>

<div id="responseBox">Odgovori ƒáe se pojavljivati ovdje...</div>

<script>
const btn = document.getElementById("talkBtn");
const box = document.getElementById("responseBox");

// Speech-to-text
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = "bs-BA";
recognition.interimResults = false;

// When user clicks button
btn.onclick = () => {
    recognition.start();
    box.innerHTML = "Slu≈°am...";
};

// When speech is recognized
recognition.onresult = async (e) => {
    const text = e.results[0][0].transcript;
    box.innerHTML = "ƒåuo sam: " + text + "<br>Dobijam odgovor...";

    // Send to AI (ChatGPT API)
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "Authorization": "Bearer YOUR_API_KEY_HERE"
        },
        body: JSON.stringify({
            model: "gpt-4o-mini",
            messages: [{ role: "user", content: text }]
        })
    });

    const data = await response.json();
    const answer = data.choices[0].message.content;

    box.innerHTML = answer;

    // Text-to-speech
    const tts = new SpeechSynthesisUtterance(answer);
    tts.lang = "bs-BA";
    speechSynthesis.speak(tts);
};
</script>

</body>
</html>
const express = require('express');
const bodyParser = require('body-parser');
const cors = require('cors');
const { Configuration, OpenAIApi } = require('openai');

const app = express();
app.use(cors());
app.use(bodyParser.json());

// Read API key from environment variable named AIDOKTOR
// In your deployment or dev environment, set: AIDOKTOR=sk-....
const OPENAI_KEY = process.env.AIDOKTOR;
if (!OPENAI_KEY) {
  console.warn('Warning: AIDOKTOR environment variable is not set. Set process.env.AIDOKTOR to your OpenAI key.');
}

const configuration = new Configuration({ apiKey: OPENAI_KEY });
const openai = new OpenAIApi(configuration);

app.post('/api/chat', async (req, res) => {
  try {
    const { prompt } = req.body;
    if (!prompt) return res.status(400).json({ error: 'Missing prompt' });

    // Example using Chat Completion (gpt-4o-mini or gpt-4o etc. change as desired)
    const response = await openai.createChatCompletion({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: 'You are AIDOKTOR: helpful, concise medical assistant (or modify as needed).' },
        { role: 'user', content: prompt }
      ],
      max_tokens: 400
    });

    const assistantText = response.data.choices?.[0]?.message?.content || '';
    res.json({ reply: assistantText });
  } catch (err) {
    console.error('OpenAI error', err?.response?.data || err.message || err);
    res.status(500).json({ error: 'OpenAI request failed', details: err?.message || err });
  }
});

// Serve static frontend when built (optional)
app.use(express.static('public'));

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`Server listening on port ${PORT}`));

---

// File: public/index.html (frontend container)
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>AIDOKTOR Voice Q&A</title>
  </head>
  <body>
    <div id="root"></div>
    <script src="app.js"></script>
  </body>
</html>

---

// File: public/app.js (vanilla JS frontend for simplicity)
// This is a small, dependency-free frontend that uses the Web Speech API.

const startBtn = document.createElement('button');
startBtn.textContent = 'Start Listening';
const stopBtn = document.createElement('button');
stopBtn.textContent = 'Stop Listening';
stopBtn.disabled = true;
const status = document.createElement('div');
status.textContent = 'Idle';
const transcriptEl = document.createElement('div');
const replyEl = document.createElement('div');

document.body.appendChild(startBtn);
document.body.appendChild(stopBtn);
document.body.appendChild(status);
document.body.appendChild(document.createElement('hr'));
document.body.appendChild(document.createTextNode('Transcript:'));
document.body.appendChild(transcriptEl);
document.body.appendChild(document.createElement('hr'));
document.body.appendChild(document.createTextNode('Reply:'));
document.body.appendChild(replyEl);

// Check browser support
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
if (!SpeechRecognition) {
  status.textContent = 'Web Speech API not supported in this browser.';
  startBtn.disabled = true;
}

let recognition;
let listening = false;

if (SpeechRecognition) {
  recognition = new SpeechRecognition();
  recognition.lang = 'bs-BA' || 'en-US'; // change as needed
  recognition.interimResults = false;
  recognition.maxAlternatives = 1;

  recognition.onstart = () => {
    listening = true;
    status.textContent = 'Listening...';
    startBtn.disabled = true;
    stopBtn.disabled = false;
  };

  recognition.onresult = async (event) => {
    const text = event.results[0][0].transcript;
    transcriptEl.textContent = text;
    status.textContent = 'Got text, sending to AIDOKTOR...';

    try {
      const r = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt: text })
      });
      const data = await r.json();
      const reply = data.reply || data.choices?.[0]?.message?.content || 'No reply';
      replyEl.textContent = reply;
      status.textContent = 'Reply received ‚Äî speaking...';

      // Speak the reply
      const utter = new SpeechSynthesisUtterance(reply);
      utter.lang = 'bs-BA' || 'en-US';
      utter.onend = () => { status.textContent = 'Idle'; };
      speechSynthesis.speak(utter);
    } catch (e) {
      console.error(e);
      status.textContent = 'Error contacting backend';
    }
  };

  recognition.onerror = (e) => {
    console.error('Recognition error', e);
    status.textContent = 'Recognition error: ' + e.error;
  };

  recognition.onend = () => {
    listening = false;
    startBtn.disabled = false;
    stopBtn.disabled = true;
    if (status.textContent === 'Listening...') status.textContent = 'Idle';
  };
}

startBtn.addEventListener('click', () => {
  if (!recognition) return;
  recognition.start();
});
stopBtn.addEventListener('click', () => {
  if (!recognition) return;
  recognition.stop();
});

---

# Deployment & setup notes

1. Put the backend code on a server (Heroku, Render, Railway, VPS, etc.).
2. Set environment variable `AIDOKTOR` to your OpenAI API key on the server (never commit it). On Linux: `export AIDOKTOR=sk-...` or configure in the hosting dashboard.
3. If you host frontend statically, make sure it points to the backend domain (replace `/api/chat` with full URL if on different origin) and enable CORS on the backend.
4. If you truly want to store the API key in your GitHub repo (not recommended), move it to a file and ensure that file is private and ignored by Git ‚Äî but again, better to use GitHub Secrets or environment variables.

# Quick tips

- If you prefer serverless, use an API route that reads `process.env.AIDOKTOR` from your host.
- To transcribe audio on the server instead of using Web Speech API, upload raw audio and call OpenAI's speech-to-text endpoint (requires more code and file handling).

---

// End of document
